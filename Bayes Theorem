Bayes Therom

how to update the probability of a hypothesis based on new evidence.
It provides a way to calculate the conditional probability of an event, given 
prior knowledge of conditions related to the event.

formula -> P(A/B) =  P(B|A) * P(A) / p(B)

exp:-
---
Normally, only 10 out of 100 emails you get are spam. (So, 10% of emails are spam.)

But today, you see an email with the word "prize" in it.

You know that spam emails often say "prize" (maybe 80% of spam emails have that word).

But normal emails almost never say "prize" (only 1% of normal emails have that word).

---

---
    You wake up and see that the ground is wet.

    You think: "Maybe it rained last night."

    But you also remember: "Maybe someone watered the garden."

Bayes' Theorem helps you answer:
ğŸ‘‰ â€œGiven that the ground is wet, how likely is it that it rained?â€

It uses what you knew before (your guess before seeing the wet ground) and what you see 
now (the wet ground) to make a better guess.
---

-
P(Hâˆ£E) = Posterior Probability: Probability of hypothesis HH after seeing evidence EE.

P(Eâˆ£H)P(Eâˆ£H) = Likelihood: Probability of evidence EE assuming hypothesis HH is true.

P(H)P(H) = Prior Probability: Initial probability of hypothesis HH before seeing evidence.

P(E)P(E) = Marginal Likelihood (or Evidence): Total probability of evidence EE under all possible hypotheses.


f P(E)P(E) is expanded, it can be written as:
P(E)=   P(E|H) Ã— P(H) + P (E|Â¬H) Ã— P (Â¬H)

Â¬H means "not H" (the hypothesis is false).

P(E|Â¬H) is the likelihood of the evidence when the hypothesis is false.

P(Â¬H) is the prior probability that the hypothesis is false.








